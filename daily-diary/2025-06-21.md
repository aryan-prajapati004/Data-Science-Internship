# ðŸ“… Date: 2025-06-21
**Project focus:** Implementing Backpropagation & Weekly Mentor Update

## Goals for today
- Implement the backpropagation logic for the Softmax and Max-Pooling layers.

- Begin implementing the backward pass for the Convolutional layer.

- Prepare and conduct the weekly progress update with my mentor.

## Tasks completed
- Implemented the backward pass for the Softmax and MaxPool2 layers, understanding how gradients flow through each. Â  

- Started working on the convolutional layer's backpropagation, which is the most mathematically intensive part. Â  

## Weekly Mentor Update:
 *Had our weekly progress meeting with Dr. Sandha to discuss the initial literature review and the challenges with backpropagation. Following the meeting, I sent a detailed email summary of the week's work, covering my study of the MobileNetV2 and EfficientNet papers and the progress on the from-scratch MNIST CNN.*

## Challenges & learnings
- The math behind backpropagation is complex but logical. Working through it step-by-step is essential.

- Understanding how gradients flow (or don't flow) through layers like Max-Pooling provides deep insight into the learning process.

## Plan for tomorrow
- Complete the backpropagation implementation for the convolutional layer and start the training loop.

## Related project files updated
- ``projects/mnist-cnn-from-scratch/cnn.py``

- ``docs/weekly_reports/week_1_summary.md``
